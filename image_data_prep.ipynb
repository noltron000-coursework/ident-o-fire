{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ðŸ”¥ Ident-O-Fire 2020\n",
    "This image recognition data-model helps determine whether or not the input contains flames.\n",
    "At first glance, this might not seem useful, and perhaps insultind due to the climate in Australia.\n",
    "\n",
    "However, the *Ident-O-Fire 2020* could be used to help solve fire emergencies through security  technology.\n",
    "For example, a security camera could run a recent image through the *Ident-O-Fire 2020* and alert admins if one or several cameras are tripped.\n",
    "\n",
    "Through this presentation, you will understand how this module was created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Step 1:** Setup\n",
    "We need to set up the dataframe to initial state.\n",
    "Then we can manipulate it more in later steps, by cleaning it, digesting it, etc.\n",
    "\n",
    "---\n",
    "\n",
    "Definitions:\n",
    "- a *dataframe* is a glorified spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Import Packages\n",
    "> \"If I have seen further, it is by standing on the shoulders of Giants.\"<br />\n",
    "> &mdash; Isaac Newton\n",
    "\n",
    "---\n",
    "\n",
    "Definitions:\n",
    "- a *package* is a project created by someone else, and is created to be utilized by others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# glob crawler for reading files and folders\n",
    "import glob\n",
    "\n",
    "# regex parser for advanced string-parsing\n",
    "import re\n",
    "\n",
    "# pandas used for dataframe manipulation\n",
    "import pandas\n",
    "\n",
    "# numpy is used for odds and ends, like high-efficiency arrays\n",
    "import numpy\n",
    "\n",
    "# python imaging library; used for deconstructing images\n",
    "from PIL import Image\n",
    "\n",
    "# other imports are described as they are used\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Crawl Datafiles\n",
    "The **glob** package is great and sets up our project for success.\n",
    "Be sure you have your directory-structure set up correctly as stated in the README&hellip;\n",
    "Otherwise, I cannot guarentee that this project will work for you!\n",
    "\n",
    "---\n",
    "\n",
    "Definitions:\n",
    "- a *datafile* is just an image file in this project; this is a colloquial term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# get all files from dirs in the data dir\n",
    "files = glob.glob('data/*/*.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# prepare dataset in an array\n",
    "dataset = []\n",
    "\n",
    "# loop through every file that \"glob\" found.\n",
    "for filepath in files:\n",
    "\t# regex used for Windows/MacOS compatibility\n",
    "\tfilecrawl = re.split(r'\\\\+|/+', filepath)\n",
    "\n",
    "\t# remove the \"data\" folder entry; its not needed\n",
    "\tfilecrawl = filecrawl[1:]\n",
    "\n",
    "\t# tag images from the fire-images folder as \"fire\"\n",
    "\tif filecrawl[0] == 'Fire images':\n",
    "\t\tfilecrawl.append(1)\n",
    "\telse:\n",
    "\t\tfilecrawl.append(0)\n",
    "\n",
    "\t# add filecrawl findings to dataset\n",
    "\tdataset.append(filecrawl)\n",
    "\t# ==NOTE==\n",
    "\t# Because the \"glob\" package arbitrarily crawls files, the\n",
    "\t# index of an item may not be the same each time this is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create our project's main dataframe\n",
    "dataframe = pandas.DataFrame(dataset, columns=['folder', 'filename', 'fire'])\n",
    "\n",
    "# display shows these tables neatly, shown below\n",
    "display(dataframe.head(), dataframe.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2: Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Check for Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# check for any duplicate filenames in the dataframe\n",
    "duplicates = dataframe['filename'].duplicated(keep=False)\n",
    "\n",
    "# Select rows with duplicate filenames\n",
    "duplicate_rows = dataframe[duplicates]\n",
    "display(duplicate_rows)\n",
    "\n",
    "# create a warning if duplicates exist\n",
    "if duplicates.sum() != 0:\n",
    "\twarning = Warning(\n",
    "\t\tf'There are {duplicates.sum()}'\n",
    "\t\t' duplicated filenames in the dataframe.'\n",
    "\t\t' proceed with caution.'\n",
    "\t) \n",
    "\tdisplay(warning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Rebalance Datapoints\n",
    "I used [this article][rebalancing] to help figure things out.\n",
    "\n",
    "[rebalancing]: https://towardsdatascience.com/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# == FIXME ==\n",
    "# this does not seem to be working properly at the moment\n",
    "\n",
    "\"\"\"\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# create an object of the classifier, called \"rebalancinator\"\n",
    "rebalancinator = BalancedBaggingClassifier(\n",
    "\tbase_estimator=DecisionTreeClassifier(),\n",
    "\tsampling_strategy='auto',\n",
    "\treplacement=False,\n",
    "\trandom_state=0\n",
    ")\n",
    "\n",
    "'''\n",
    "# train the classifier.\n",
    "rebalancinator.fit(x_train, y_train)\n",
    "preds = rebalancinator.predict(x_train)\n",
    "'''\n",
    "\"\"\"\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train/Test Split\n",
    "In datascience it is common practice to split your data into four main sections: your training data, your testing data, your input/open/x data, and your output/hidden/y data.\n",
    "Combining these will give you x_train, y_train, x_test, and y_test.\n",
    "\n",
    "The model is run on x_train and y_train.\n",
    "Once the model is complete, you can test the model's accuracy on x_test and y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# for x: keep the dataframe but drop the fire column\n",
    "x = dataframe.drop(columns=['fire'])\n",
    "\n",
    "# for y: drop everything in the dataframe but fire\n",
    "y = dataframe.loc[:, ['fire']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use the built-in \"train test split\" function\n",
    "# to generate the four desireable segments of data.\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "\tx, y, test_size=0.35)\n",
    "\n",
    "x_train = x_train.reset_index().drop('index', 1)\n",
    "y_train = y_train.reset_index().drop('index', 1)\n",
    "x_test = x_test.reset_index().drop('index', 1)\n",
    "y_test = y_test.reset_index().drop('index', 1)\n",
    "\n",
    "display(x_test.head())\n",
    "display(y_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create Image Vectors\n",
    "An Image Vector is an array of pixels and their intensity values (from 0 to 255).\n",
    "If an image is 86x42 pixels, the array will have the same size.\n",
    "However the image produces three arrays of this size, one of each color (red, green, & blue).\n",
    "\n",
    "---\n",
    "\n",
    "Definitions:\n",
    "- an *image vector* is a 3D-array \n",
    "    - two dimensions are for an image's length and width, representing each pixel\n",
    "    - the third dimension represents the three color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "image_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_img_vector(x, index):\n",
    "\t# find filepath\n",
    "\tfilename = x['filename'][index]\n",
    "\tfolder = x['folder'][index]\n",
    "\tfilepath = f'data/{folder}/{filename}'\n",
    "\n",
    "\t# open the image via its filepath\n",
    "\timg = Image.open(filepath)\n",
    "\t# ==NOTE==\n",
    "\t# the Image class was imported from PIL\n",
    "\t# (python image library)\n",
    "\n",
    "\t# remove transparency layer\n",
    "\timg = img.convert('RGB')\n",
    "\n",
    "\t# resize the image\n",
    "\timg = img.resize((image_length, image_length))\n",
    "\n",
    "\t# return the image vector\n",
    "\treturn img_to_array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This data generator creates image vectors carefully in batches.\n",
    "For example, if there are 500 data entries, we will run out of memory because our image-vectors are so large.\n",
    "However if we reduce this to several batches of 50, the computer will have sufficient memory to complete each batch one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def data_gen(x, y, batch_size):\n",
    "\t# n_batch variables are empty arrays of constant size.\n",
    "\t# x_batch is has RGB values for each pixel's coordinate.\n",
    "\t# y_batch represents whether there is fire or not (0/1).\n",
    "\tx_batch = numpy.zeros((batch_size, image_length, image_length, 3))\n",
    "\ty_batch = numpy.zeros((batch_size, 1))\n",
    "\n",
    "\t# loop through entire dataframe, index by index\n",
    "\tfor index in range(len(x)):\n",
    "\t\t# using batch_size, we can determine \n",
    "\t\tx_batch[index % batch_size] = get_img_vector(x, index)\n",
    "\t\ty_batch[index % batch_size] = y['fire'][index]\n",
    "\n",
    "\t\t# if there has been {batch_size} items, we yield.\n",
    "\t\t# the last batch is an outlier; its batch is smaller.\n",
    "\t\tif ((index + 1) % batch_size == 0\n",
    "\t\tor (index + 1) == len(dataframe)):\n",
    "\t\t\tyield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# use \"sequential\" mode from keras module\n",
    "# see https://jovianlin.io/keras-models-sequential-vs-functional/\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Neural Network Example\n",
    "![nn][nn]\n",
    "\n",
    "[nn]: ./images/neural_network.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conv2D Example\n",
    "![conv2D_01][conv2D_01]\n",
    "\n",
    "[conv2D_01]: ./images/conv2D_01.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conv2D Example\n",
    "![conv2D_02][conv2D_02]\n",
    "\n",
    "[conv2D_02]: ./images/conv2D_02.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Flatten Examples\n",
    "![flatten_01][flatten_01]\n",
    "\n",
    "[flatten_01]: ./images/flatten_01.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# prepare the model\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', input_shape=(image_length, image_length, 3)))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Conv2D(32, (2, 2), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(\n",
    "\tloss=binary_crossentropy,\n",
    "\toptimizer=Adadelta(),\n",
    "\tmetrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\tprint(f\"\\t\\tEPOCH {epoch+1}/{epochs}\")\n",
    "\tfor x_batch, y_batch in data_gen(x_train, y_train, batch_size):\n",
    "\t\tmodel.train_on_batch(\n",
    "\t\t\tx_batch, y_batch\n",
    "\t\t)\n",
    "\t\tloss, accuracy = model.evaluate(x_batch, y_batch)\n",
    "\t\tprint('accuracy:', accuracy)\n",
    "# Possibly something wrong with backpropogation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST. Possibly delete later if no results\n",
    "# epochs = 50\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "# \tprint(f\"\\t\\tEPOCH {epoch+1}/{epochs}\")\n",
    "# \tfor x_batch, y_batch in data_gen(x_train, y_train, batch_size):\n",
    "# \t\tmodel.fit(\n",
    "# \t\t\tx_batch, y_batch \n",
    "# \t\t)\n",
    "# \t\tloss, accuracy = model.evaluate(x_batch, y_batch)\n",
    "# \t\tprint('accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Step 4: Affirm Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_y_value(index):\n",
    "\treturn y_test['fire'][index]\n",
    "\n",
    "def get_y_guess(index):\n",
    "\tfire = model.predict(get_img_vector(x_test, index).reshape(-1, image_length, image_length, 3))\n",
    "\tif fire > 0:\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "def get_confusion_matrix_objects():\n",
    "\ty_values = []\n",
    "\ty_guesses = []\n",
    "\tfor index in list(range(len(x_test))):\n",
    "\t\ty_values.append(get_y_value(index))\n",
    "\t\ty_guesses.append(get_y_guess(index))\n",
    "\treturn (y_values, y_guesses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "tn, fp, fn, tp = confusion_matrix(*get_confusion_matrix_objects()).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "\tf' true positive {tp}\\n'\n",
    "\tf' true negative {tn}\\n'\n",
    "\tf'false positive {fp}\\n'\n",
    "\tf'false negative {fn}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for index in list(range(len(x_test))):\n",
    "\tpredict = model.predict(get_img_vector(x_test, index).reshape(-1, image_length, image_length, 3))\n",
    "\tif predict[0][0] > 0:\n",
    "\t\tprint(\n",
    "\t\t\tf'\\n{predict[0][0]} \\n' \n",
    "\t\t\t'<img src=\"./data/'\n",
    "\t\t\tf'{x_train[\"folder\"][index]}'\n",
    "\t\t\t'/'\n",
    "\t\t\tf'{x_train[\"filename\"][index]}\">'\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
